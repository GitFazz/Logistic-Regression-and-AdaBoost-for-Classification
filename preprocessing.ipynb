{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# information gain to find the best feature\n",
    "def info_gain(X,y,f) :\n",
    "    # find unique values in the feature\n",
    "    unique_vals = np.unique(X[f])\n",
    "    # find entropy of the feature\n",
    "    entropy_f = 0\n",
    "    for i in unique_vals :\n",
    "        entropy_f += (X[f] == i).sum()/X.shape[0]*np.log2((X[f] == i).sum()/X.shape[0])\n",
    "    # find entropy of the dataset\n",
    "    entropy_d = 0\n",
    "    for i in np.unique(y) :\n",
    "        entropy_d += (y == i).sum()/y.shape[0]*np.log2((y == i).sum()/y.shape[0])\n",
    "    # find information gain\n",
    "    return entropy_d - entropy_f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tanh activation function\n",
    "def tanh(x) :\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "# sklearn confusion matrix\n",
    "def confusion_matrix(y_true,y_pred) :\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    return confusion_matrix(y_true,y_pred)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true,y_pred) :\n",
    "    \n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Accuracy : \",accuracy)\n",
    "    print(\"Precision : \",precision)\n",
    "    print(\"Recall : \",recall)\n",
    "    print(\"F1 Score : \",f1)\n",
    "    print(\"_____________________________________________________\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# logistic error \n",
    "\n",
    "def error(y,h) :\n",
    "    return np.sum((y-h)**2)/y.shape[0]\n",
    "\n",
    "# logistic regression implementation\n",
    "\n",
    "def logistic_regression(X,y,iteration=1000,alpha=0.01):\n",
    "    \n",
    "    # initialize the weights\n",
    "    w = np.zeros(X.shape[1])\n",
    "     \n",
    "    for i in range(iteration):\n",
    "        # hypothesis\n",
    "        h = tanh(np.dot(X,w.T))\n",
    "\n",
    "        # update weights\n",
    "       \n",
    "        temp1 = y - h\n",
    "        temp2 = 1 - h**2\n",
    "        grad = np.dot(X.T,temp1*temp2)/ X.shape[0]\n",
    "        w = w + alpha*grad\n",
    "\n",
    "        #print(error(y,h))\n",
    "       \n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "# resample data according to the weights\n",
    "\n",
    "def resample(X,y,w) :\n",
    "    # X, y : examples\n",
    "    # w : weights\n",
    "\n",
    "    # initialize the new dataset\n",
    "    X_new = np.zeros(X.shape)\n",
    "    y_new = np.zeros(y.shape)\n",
    "\n",
    "    # find the index of the examples to be resampled\n",
    "    index = np.random.choice(X.shape[0],X.shape[0],replace=True,p=w)\n",
    "    \n",
    "\n",
    "    # resample the examples\n",
    "    for i in range(X.shape[0]):\n",
    "        X_new[i] = X.iloc[index[i]]\n",
    "        y_new[i] = y.iloc[index[i]]\n",
    "    \n",
    "\n",
    "    return X_new, y_new\n",
    "\n",
    "def AdaBoost(X,y,K) :\n",
    "    # X, y : examples\n",
    "    # K : number of hypothesis in the ensemble\n",
    "\n",
    "    # initialize the weights\n",
    "    w = np.ones(X.shape[0])/X.shape[0]\n",
    "\n",
    "    # initialize the hypothesis vector\n",
    "    h = []\n",
    "\n",
    "    # initialize the hypothesis weight vector\n",
    "    z = []\n",
    "\n",
    "    for i in range(K) :\n",
    "        # resample the data according to the weights\n",
    "        X_new, y_new = resample(X,y,w)\n",
    "   \n",
    "        # Kth hypothesis\n",
    "        Th = logistic_regression(X_new,y_new)\n",
    "\n",
    "        # find the error\n",
    "        error = 0\n",
    "\n",
    "        for j in range(X.shape[0]) :\n",
    "            if y_new[j] != np.sign(np.dot(X_new[j],Th.T)) :\n",
    "                error += w[j]\n",
    "            \n",
    "        if error > 0.5 :\n",
    "            continue\n",
    "\n",
    "        for j in range(X.shape[0]) :\n",
    "            if y_new[j] == np.sign(np.dot(X_new[j],Th.T)) :\n",
    "                w[j] = w[j]*error/(1-error)\n",
    "        \n",
    "        # normalize the weights\n",
    "        w = w/np.sum(w)\n",
    "\n",
    "        # append the hypothesis and its weight\n",
    "        h.append(Th)\n",
    "        z.append((1-error)/error)\n",
    "        print(\"Ensemble : \",i+1, \"weight : \",z[i])\n",
    "    \n",
    "        \n",
    "    return weighted_majority(h,z)\n",
    "\n",
    "\n",
    "def weighted_majority(h,z) :\n",
    "    h_new = np.zeros(h[0].shape)\n",
    "    for i in range(len(z)) :\n",
    "        h_new += z[i]*h[i]\n",
    "\n",
    "    return h_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble :  1 weight :  3.9385425812116153\n",
      "Ensemble :  2 weight :  3.905775446610879\n",
      "Ensemble :  3 weight :  4.08505678470163\n",
      "Ensemble :  4 weight :  3.8875477813184274\n",
      "Ensemble :  5 weight :  4.012056798179578\n",
      "Ensemble :  6 weight :  4.455599664559551\n",
      "Ensemble :  7 weight :  3.8453443860954537\n",
      "Ensemble :  8 weight :  4.073617436603568\n",
      "Ensemble :  9 weight :  4.8652838512249375\n",
      "Ensemble :  10 weight :  5.118234884269043\n",
      "[[946  92]\n",
      " [207 162]]\n",
      "-----------------------------------------------------\n",
      "Accuracy :  0.7874911158493249\n",
      "Precision :  0.6377952755905512\n",
      "Recall :  0.43902439024390244\n",
      "F1 Score :  0.5200642054574639\n",
      "_____________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# read dataset 1\n",
    "dataset1 = pd.read_csv(\"dataset_1.csv\")\n",
    "dataset1.head()\n",
    "\n",
    "# drop the first column\n",
    "dataset1.drop(['customerID'], axis=1, inplace=True)\n",
    "\n",
    "dataset1['TotalCharges'] = pd.to_numeric(dataset1['TotalCharges'] , errors='coerce')\n",
    "dataset1.TotalCharges.dtype\n",
    "\n",
    "# covert the label to -1 and 1\n",
    "dataset1['Churn'] = dataset1['Churn'].apply(lambda x: -1 if x == 'No' else 1)\n",
    "\n",
    "# add bias column as first column\n",
    "dataset1.insert(0, 'bias', 1)\n",
    "\n",
    "\n",
    "\n",
    "# normalize the data\n",
    "dataset1['tenure'] = (dataset1['tenure'] - dataset1['tenure'].mean())/dataset1['tenure'].std()\n",
    "dataset1['MonthlyCharges'] = (dataset1['MonthlyCharges'] - dataset1['MonthlyCharges'].mean())/dataset1['MonthlyCharges'].std()\n",
    "dataset1['TotalCharges'] = (dataset1['TotalCharges'] - dataset1['TotalCharges'].mean())/dataset1['TotalCharges'].std()\n",
    "\n",
    "# find categorical variables\n",
    "cat_vars = []\n",
    "for i in dataset1.columns :\n",
    "    if dataset1[i].dtype == 'object' :\n",
    "        cat_vars.append(i)\n",
    "\n",
    "# convert categorical variables to integers\n",
    "for i in cat_vars :\n",
    "    dataset1[i] = dataset1[i].astype('category').cat.codes\n",
    "\n",
    "\n",
    "# one hot encoding for categorical variables with less than 5 unique values\n",
    "for i in cat_vars :\n",
    "    if dataset1[i].nunique() < 3 :\n",
    "        dataset1 = pd.get_dummies(dataset1,drop_first=True,columns=[i])\n",
    "\n",
    "# drop rows with null values\n",
    "dataset1.dropna(inplace=True)\n",
    "\n",
    "# X and y\n",
    "y = dataset1['Churn']\n",
    "X = dataset1.drop(['Churn'],axis=1)\n",
    "\n",
    "\n",
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "model = AdaBoost(X_train,y_train,10)\n",
    "\n",
    "h = np.dot(X_test,model.T)\n",
    "y_pred = np.sign(h).astype(int)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "evaluate_model(y_test,y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6f8c3de90b578c42c5dca1ace0652505f65d16b2ece0eaf147a9cf03f55f9e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
